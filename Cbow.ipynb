{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing import sequence\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from spellchecker import SpellChecker\n",
    "import Levenshtein\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load the GloVe word embeddings\n",
    "# def load_embeddings(embedding_file):\n",
    "#     embeddings_index = {}\n",
    "#     with open(embedding_file, encoding='utf-8') as f:\n",
    "#         for line in f:\n",
    "#             values = line.split()\n",
    "#             word = values[0]\n",
    "#             coefs = np.asarray(values[1:], dtype='float32')\n",
    "#             embeddings_index[word] = coefs\n",
    "#     return embeddings_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define a function to predict the next word given a context\n",
    "# def predict_next_word(context):\n",
    "#     context_words = context.split()\n",
    "#     context_embedding = np.zeros((len(embeddings_index[next(iter(embeddings_index))]),))\n",
    "#     for word in context_words:\n",
    "#         if word in embeddings_index:\n",
    "#             context_embedding += embeddings_index[word]\n",
    "#     context_embedding /= len(context_words)\n",
    "    \n",
    "#     similarities = {}\n",
    "#     for word in embeddings_index.keys():\n",
    "#         if word not in context_words:\n",
    "#             word_embedding = embeddings_index[word]\n",
    "#             sim = cosine_similarity([context_embedding], [word_embedding])[0][0]\n",
    "#             similarities[word] = sim\n",
    "            \n",
    "#     top_3_words = sorted(similarities.items(), key=lambda x: x[1], reverse=True)[:3]\n",
    "#     return [w[0] for w in top_3_words]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load GloVe word vectors\n",
    "def load_word_vectors(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        words = set()\n",
    "        word_to_vec_map = {}\n",
    "        for line in f:\n",
    "            line = line.strip().split()\n",
    "            curr_word = line[0]\n",
    "            words.add(curr_word)\n",
    "            word_to_vec_map[curr_word] = np.array(line[1:], dtype=np.float64)\n",
    "    return words, word_to_vec_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "words, word_to_vec_map = load_word_vectors(\"glove.6B.100d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def spell_check(text, vocab):\n",
    "    list_of_words = text.split()\n",
    "    misspelled_words = []\n",
    "    for word in list_of_words:\n",
    "        if word not in vocab:\n",
    "            misspelled_words.append(word)\n",
    "    return misspelled_words    \n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correct_words(misspelled_words):\n",
    "    spell = SpellChecker()\n",
    "    list_of_candidate_words = []\n",
    "    for word in misspelled_words:\n",
    "        candidate_words = spell.candidates(word)\n",
    "        list_of_candidate_words.append([word, candidate_words])\n",
    "    return list_of_candidate_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def choose_the_best_candidate(list_of_candidate_words):\n",
    "    distances = []\n",
    "    correct_word = []\n",
    "    for item in list_of_candidate_words:\n",
    "        for cadidate in item[1]:\n",
    "            distances.append(Levenshtein.distance(item[0], cadidate))\n",
    "        max_value = max(distances)\n",
    "        max_index = distances.index(max_value)\n",
    "        correct_word.append([item[0], list(item[1])[max_index]])\n",
    "    return correct_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['forsoken', 'laaand', 'beyon']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"you aree forsoken in the laaand beyon the fog\"\n",
    "misspelled_words = spell_check(text=text, vocab=words)\n",
    "misspelled_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['forsoken', {'forsaken'}],\n",
       " ['laaand',\n",
       "  {'ahand',\n",
       "   'anand',\n",
       "   'baaad',\n",
       "   'carand',\n",
       "   'dayand',\n",
       "   'laaa',\n",
       "   'lagaan',\n",
       "   'lamond',\n",
       "   'land',\n",
       "   'lazard',\n",
       "   'leland',\n",
       "   'lnland',\n",
       "   'lsland'}],\n",
       " ['beyon', {'bayon', 'belon', 'beon', 'beton', 'beyond', 'beyong'}]]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_candidate_words = correct_words(misspelled_words=misspelled_words)\n",
    "list_of_candidate_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['forsoken', 'forsaken'], ['laaand', 'leland'], ['beyon', 'beton']]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_word = choose_the_best_candidate(list_of_candidate_words=list_of_candidate_words)\n",
    "correct_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'beyond'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spell = SpellChecker()\n",
    "spell.correction(\"beyon\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
