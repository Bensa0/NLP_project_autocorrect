{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from keras.models import load_model\n",
    "from cbow_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "window_size = 2 \n",
    "window_size_corpus = window_size*2\n",
    "\n",
    "# Set numpy seed for reproducible results\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 4, 100)            255700    \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2557)              258257    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 513,957\n",
      "Trainable params: 513,957\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#cbow_model = cbow_model(vocab_size = V, embedding_dim = 100 , window_size = window_size)\n",
    "cbow_model = load_model('my_model.h5')\n",
    "cbow_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2556\n"
     ]
    }
   ],
   "source": [
    "file_name = 'ALICE.txt'\n",
    "corpus = open(file_name).readlines()\n",
    "\n",
    "# Remove sentences with fewer than 3 words\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "# Remove punctuation in text and fit tokenizer on entire corpus\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# Convert text to sequence of integer values\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "n_samples = sum(len(s) for s in corpus) # Total number of words in the corpus\n",
    "V = len(tokenizer.word_index) + 1 # Total number of unique words in the corpus\n",
    "my_dict = tokenizer.word_index\n",
    "\n",
    "vocab = []\n",
    "for key, value in my_dict.items():\n",
    "    vocab.append(key)\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'ALICE.txt'\n",
    "corpus_lst = open(file_name).readlines()\n",
    "# Remove sentences with fewer than 3 words\n",
    "corpus_lst = [sentence for sentence in corpus_lst if sentence.count(\" \") >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words =\" \".join(corpus_lst).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['another', 'wer', 'wer', 'wer', 'went']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the vocabulary\n",
    "vocabulary = vocab\n",
    "\n",
    "# Define a context of words\n",
    "context = 'another wer wer  wer went'.split()\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.70710678, 0.        , 0.        , 0.        , 0.70710678])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the context to a vector\n",
    "context_vector = np.zeros((len(context),))\n",
    "for word in context:\n",
    "    if word in vocabulary:\n",
    "        context_vector[context.index(word)] += 1\n",
    "# Normalize the context vector\n",
    "context_vector /= np.linalg.norm(context_vector)\n",
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7071067811865475\n",
      "0.7071067811865475\n"
     ]
    }
   ],
   "source": [
    "for n in context_vector:\n",
    "    if n != 0 :\n",
    "        print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [3, 1],\n",
       "       [2, 3]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = np.array([[1, 2, 3, 1, 2, 3]])\n",
    "n.reshape(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 5 into shape (2556,5)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Reshape the context vector to have a shape of (1, len(vocabulary))\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m context_vector \u001b[39m=\u001b[39m context_vector\u001b[39m.\u001b[39;49mreshape(\u001b[39mlen\u001b[39;49m(vocab) , \u001b[39mlen\u001b[39;49m(context))\n\u001b[0;32m      3\u001b[0m context_vector\n",
      "\u001b[1;31mValueError\u001b[0m: cannot reshape array of size 5 into shape (2556,5)"
     ]
    }
   ],
   "source": [
    "# Reshape the context vector to have a shape of (1, len(vocabulary))\n",
    "context_vector = context_vector.reshape(len(vocab) , len(context))\n",
    "context_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 5)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "context_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"c:\\Users\\basel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\basel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\basel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\basel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\basel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\basel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 4), found shape=(None, 5)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Predict the target word\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m prediction \u001b[39m=\u001b[39m cbow_model\u001b[39m.\u001b[39;49mpredict(context_vector)\n\u001b[0;32m      4\u001b[0m \u001b[39m# Get the index of the predicted word\u001b[39;00m\n\u001b[0;32m      5\u001b[0m predicted_index \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(prediction)\n",
      "File \u001b[1;32mc:\\Users\\basel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Temp\\__autograph_generated_fileqta82e6e.py:15\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf__predict_function\u001b[1;34m(iterator)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     14\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m---> 15\u001b[0m     retval_ \u001b[39m=\u001b[39m ag__\u001b[39m.\u001b[39mconverted_call(ag__\u001b[39m.\u001b[39mld(step_function), (ag__\u001b[39m.\u001b[39mld(\u001b[39mself\u001b[39m), ag__\u001b[39m.\u001b[39mld(iterator)), \u001b[39mNone\u001b[39;00m, fscope)\n\u001b[0;32m     16\u001b[0m \u001b[39mexcept\u001b[39;00m:\n\u001b[0;32m     17\u001b[0m     do_return \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: in user code:\n\n    File \"c:\\Users\\basel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2137, in predict_function  *\n        return step_function(self, iterator)\n    File \"c:\\Users\\basel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2123, in step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"c:\\Users\\basel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2111, in run_step  **\n        outputs = model.predict_step(data)\n    File \"c:\\Users\\basel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\training.py\", line 2079, in predict_step\n        return self(x, training=False)\n    File \"c:\\Users\\basel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\utils\\traceback_utils.py\", line 70, in error_handler\n        raise e.with_traceback(filtered_tb) from None\n    File \"c:\\Users\\basel\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\engine\\input_spec.py\", line 295, in assert_input_compatibility\n        raise ValueError(\n\n    ValueError: Input 0 of layer \"sequential\" is incompatible with the layer: expected shape=(None, 4), found shape=(None, 5)\n"
     ]
    }
   ],
   "source": [
    "# Predict the target word\n",
    "prediction = cbow_model.predict(context_vector)\n",
    "\n",
    "# Get the index of the predicted word\n",
    "predicted_index = np.argmax(prediction)\n",
    "\n",
    "# Get the predicted word from the vocabulary\n",
    "predicted_word = vocabulary[predicted_index]\n",
    "\n",
    "print(\"The predicted next word is:\", predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m model\u001b[39m.\u001b[39mget_layer(\u001b[39m'\u001b[39m\u001b[39membedding\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mget_weights()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "model.get_layer('embedding').get_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2557, 100)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = cbow_model.get_layer('embedding').get_weights()[0]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>90</th>\n",
       "      <th>91</th>\n",
       "      <th>92</th>\n",
       "      <th>93</th>\n",
       "      <th>94</th>\n",
       "      <th>95</th>\n",
       "      <th>96</th>\n",
       "      <th>97</th>\n",
       "      <th>98</th>\n",
       "      <th>99</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.28074</td>\n",
       "      <td>-0.898299</td>\n",
       "      <td>-0.114706</td>\n",
       "      <td>0.358534</td>\n",
       "      <td>-0.505169</td>\n",
       "      <td>0.03178</td>\n",
       "      <td>0.390315</td>\n",
       "      <td>0.155193</td>\n",
       "      <td>1.063202</td>\n",
       "      <td>0.78178</td>\n",
       "      <td>...</td>\n",
       "      <td>0.553495</td>\n",
       "      <td>0.258711</td>\n",
       "      <td>0.472479</td>\n",
       "      <td>0.351076</td>\n",
       "      <td>-0.627747</td>\n",
       "      <td>-0.853452</td>\n",
       "      <td>-0.049117</td>\n",
       "      <td>0.187064</td>\n",
       "      <td>-0.875002</td>\n",
       "      <td>0.40091</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 100 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        0         1         2         3         4        5         6    \n",
       "0 -0.28074 -0.898299 -0.114706  0.358534 -0.505169  0.03178  0.390315  \\\n",
       "\n",
       "         7         8        9   ...        90        91        92        93   \n",
       "0  0.155193  1.063202  0.78178  ...  0.553495  0.258711  0.472479  0.351076  \\\n",
       "\n",
       "         94        95        96        97        98       99  \n",
       "0 -0.627747 -0.853452 -0.049117  0.187064 -0.875002  0.40091  \n",
       "\n",
       "[1 rows x 100 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(x).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save embeddings for vectors of length 50, 150 and 300 using cbow model\n",
    "weights = cbow_model.get_weights()\n",
    "\n",
    "# Get the embedding matrix\n",
    "embedding = weights[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2557, 100)"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.metrics.pairwise import cosine_distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed(word, embedding =cbow_model.get_weights()[0] , vocab_size=V, tokenizer=tokenizer):\n",
    "    \"\"\" Embed a word by getting the one hot encoding and taking the dot product of this vector with the \n",
    "        embedding matrix 'word' = string type\n",
    "    \"\"\"\n",
    "    # get the index of the word from the tokenizer, i.e. convert the string to it's corresponding integer in the vocabulary\n",
    "    int_word = tokenizer.texts_to_sequences([word])[0]\n",
    "    # get the one-hot encoding of the word\n",
    "    bin_word = to_categorical(int_word, V)\n",
    "    return np.dot(bin_word, embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embed('alice')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['another', 'moment', 'down', 'went']"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a context of words\n",
    "context = 'another moment down went'.split()\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 38ms/step\n",
      "The predicted next word is: people\n"
     ]
    }
   ],
   "source": [
    "#Convert the context to a vector\n",
    "context_vector = np.zeros((len(vocab),))\n",
    "for word in context:\n",
    "    if embed(word) in embedding:\n",
    "        context_vector[vocab.index(word)] += 1\n",
    "\n",
    "# Normalize the context vector\n",
    "context_vector /= np.linalg.norm(context_vector)\n",
    "\n",
    "# Reshape the context vector to have a shape of (1, len(vocabulary))\n",
    "context_vector = context_vector.reshape(1, len(vocab))\n",
    "context_vector\n",
    "# Predict the target word\n",
    "prediction = cbow_model.predict(context_vector)[0]\n",
    "\n",
    "# Get the index of the predicted word\n",
    "predicted_index = np.argmax(prediction)\n",
    "\n",
    "# Get the predicted word from the vocabulary\n",
    "predicted_word = vocab[predicted_index]\n",
    "\n",
    "print(\"The predicted next word is:\", predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in vocab:\n",
    "    if w =='In':\n",
    "        print('w')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
