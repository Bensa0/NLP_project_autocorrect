{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import operator\n",
    "from tensorflow import keras\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors as nn\n",
    "from matplotlib import pylab\n",
    "import pandas as pd\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Embedding, Lambda\n",
    "import keras.backend as K\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.models import load_model\n",
    "from cbow_model import cbow_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'ALICE.txt'\n",
    "corpus = open(file_name).readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(27165, 2557)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remove sentences with fewer than 3 words\n",
    "corpus = [sentence for sentence in corpus if sentence.count(\" \") >= 2]\n",
    "\n",
    "# Remove punctuation in text and fit tokenizer on entire corpus\n",
    "tokenizer = Tokenizer(filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~\\t\\n'+\"'\")\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "\n",
    "# Convert text to sequence of integer values\n",
    "corpus = tokenizer.texts_to_sequences(corpus)\n",
    "n_samples = sum(len(s) for s in corpus) # Total number of words in the corpus\n",
    "V = len(tokenizer.word_index) + 1 # Total number of unique words in the corpus\n",
    "\n",
    "n_samples, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2556\n"
     ]
    }
   ],
   "source": [
    "my_dict = tokenizer.word_index\n",
    "\n",
    "vocab = []\n",
    "for key, value in my_dict.items():\n",
    "    vocab.append(key)\n",
    "\n",
    "print(len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'ALICE.txt'\n",
    "corpus_lst = open(file_name).readlines()\n",
    "# Remove sentences with fewer than 3 words\n",
    "corpus_lst = [sentence for sentence in corpus_lst if sentence.count(\" \") >= 2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "window_size = 2 \n",
    "window_size_corpus = window_size*2\n",
    "\n",
    "# Set numpy seed for reproducible results\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 4, 100)            255700    \n",
      "                                                                 \n",
      " lambda (Lambda)             (None, 100)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 2557)              258257    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 513,957\n",
      "Trainable params: 513,957\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#cbow_model = cbow_model(vocab_size = V, embedding_dim = 100 , window_size = window_size)\n",
    "cbow_model = load_model('my_model.h5')\n",
    "cbow_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing import sequence\n",
    "\n",
    "# Prepare the data for the CBOW model\n",
    "def generate_data_cbow(corpus, window_size, V):\n",
    "    all_in = []\n",
    "    all_out = []\n",
    "\n",
    "    # Iterate over all sentences\n",
    "    for sentence in corpus:\n",
    "        L = len(sentence)\n",
    "        for index, word in enumerate(sentence):\n",
    "            start = index - window_size\n",
    "            end = index + window_size + 1\n",
    "\n",
    "            # Empty list which will store the context words\n",
    "            context_words = []\n",
    "            for i in range(start, end):\n",
    "                # Skip the 'same' word\n",
    "                if i != index:\n",
    "                    # Add a word as a context word if it is within the window size\n",
    "                    if 0 <= i < L:\n",
    "                        context_words.append(sentence[i])\n",
    "                    else:\n",
    "                        # Pad with zero if there are no words \n",
    "                        context_words.append(0)\n",
    "            # Append the list with context words\n",
    "            all_in.append(context_words)\n",
    "\n",
    "            # Add one-hot encoding of the target word\n",
    "            all_out.append(to_categorical(word, V))\n",
    "                 \n",
    "    return (np.array(all_in), np.array(all_out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((27165, 4), (27165, 2557))"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the training data\n",
    "X_cbow, y_cbow = generate_data_cbow(corpus, window_size, V)\n",
    "X_cbow.shape, y_cbow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['In', 'another', 'moment', 'down', 'went']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the vocabulary\n",
    "vocabulary = vocab\n",
    "\n",
    "# Define a context of words\n",
    "context = 'In another moment down went'.split()\n",
    "context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert the context to a vector\n",
    "context_vector = np.zeros((len(vocabulary),))\n",
    "for word in context:\n",
    "    if word in vocabulary:\n",
    "        context_vector[vocabulary.index(word)] += 1\n",
    "\n",
    "# Normalize the context vector\n",
    "context_vector /= np.linalg.norm(context_vector)\n",
    "\n",
    "# Reshape the context vector to have a shape of (1, len(vocabulary))\n",
    "context_vector = context_vector.reshape(1, len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'vocabulary' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m context \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mthis\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mis\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39ma\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m      4\u001b[0m \u001b[39m# Convert the context to a vector\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m context_vector \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mzeros((\u001b[39mlen\u001b[39m(vocabulary),))\n\u001b[0;32m      6\u001b[0m \u001b[39mfor\u001b[39;00m word \u001b[39min\u001b[39;00m context:\n\u001b[0;32m      7\u001b[0m     \u001b[39mif\u001b[39;00m word \u001b[39min\u001b[39;00m vocabulary:\n",
      "\u001b[1;31mNameError\u001b[0m: name 'vocabulary' is not defined"
     ]
    }
   ],
   "source": [
    "# Define the context window\n",
    "context = [\"this\", \"is\", \"a\", \"test\"]\n",
    "\n",
    "# Convert the context to a vector\n",
    "context_vector = np.zeros((len(vocabulary),))\n",
    "for word in context:\n",
    "    if word in vocabulary:\n",
    "        context_vector[vocabulary.index(word)] += 1\n",
    "\n",
    "# Normalize the context vector\n",
    "context_vector /= np.linalg.norm(context_vector)\n",
    "\n",
    "# Reshape the context vector to have a shape of (1, len(vocabulary))\n",
    "context_vector = context_vector.reshape(1, len(vocabulary))\n",
    "\n",
    "# Predict the target word\n",
    "prediction = cbow_model.predict(context_vector)[0]\n",
    "\n",
    "# Get the index of the predicted word\n",
    "predicted_index = np.argmax(prediction)\n",
    "\n",
    "# Get the predicted word from the vocabulary\n",
    "predicted_word = vocabulary[predicted_index]\n",
    "\n",
    "print(\"The predicted next word is:\", predicted_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zhe is not in the vocabulary and can't be corrected\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# Load the CBOW model from an HDF5 file\n",
    "model = load_model('my_model.h5')\n",
    "\n",
    "# Define a misspelled word to correct\n",
    "misspelled_word = 'zhe'\n",
    "\n",
    "# Check if the misspelled word is in the vocabulary\n",
    "if misspelled_word in model.get_layer('embedding').get_weights()[0].tolist():\n",
    "    # If the misspelled word is already in the vocabulary, there's nothing to correct\n",
    "    print(f'{misspelled_word} is already in the vocabulary')\n",
    "else:\n",
    "    # If the misspelled word is not in the vocabulary, try to correct it\n",
    "    # by finding the most similar word in the vocabulary\n",
    "    # First, remove any non-alphabetic characters from the misspelled word\n",
    "    misspelled_word = re.sub('[^a-zA-Z]', '', misspelled_word)\n",
    "\n",
    "    # Check if the cleaned-up misspelled word is still not in the vocabulary\n",
    "    if misspelled_word not in model.get_layer('embedding').get_weights()[0].tolist():\n",
    "        # If the cleaned-up misspelled word is still not in the vocabulary,\n",
    "        # we can't correct it\n",
    "        print(f'{misspelled_word} is not in the vocabulary and can\\'t be corrected')\n",
    "    else:\n",
    "        # Get the embedding for the misspelled word\n",
    "        misspelled_word_embedding = model.get_layer('embedding_layer').get_weights()[0][model.get_layer('embedding').get_weights()[0].tolist().index(misspelled_word)]\n",
    "\n",
    "        # Calculate the cosine similarity between the misspelled word and all other words in the vocabulary\n",
    "        word_embeddings = model.get_layer('embedding_layer').get_weights()[0]\n",
    "        cosine_similarities = np.dot(word_embeddings, misspelled_word_embedding) / (np.linalg.norm(word_embeddings, axis=1) * np.linalg.norm(misspelled_word_embedding))\n",
    "\n",
    "        # Get the most similar word in the vocabulary to the misspelled word\n",
    "        most_similar_word_index = np.argsort(cosine_similarities)[::-1][1]\n",
    "        most_similar_word = model.get_layer('embedding').get_weights()[0][most_similar_word_index]\n",
    "\n",
    "        # Print the corrected word and its cosine similarity to the misspelled word\n",
    "        cosine_similarity = cosine_similarities[most_similar_word_index]\n",
    "        print(f'{misspelled_word} corrected to {most_similar_word} with cosine similarity {cosine_similarity}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CBOW model from the h5 file\n",
    "model = load_model('my_model.h5')\n",
    "\n",
    "# Define a function to correct a misspelled word\n",
    "def correct_spelling(word):\n",
    "    # Convert the word to lowercase\n",
    "    word = word.lower()\n",
    "    \n",
    "    # Find the nearest neighbors of the misspelled word\n",
    "    neighbors = model.(word, topn=5)\n",
    "    \n",
    "    # Extract the corrected word with the highest similarity score\n",
    "    corrected_word = neighbors[0][0]\n",
    "    \n",
    "    return corrected_word"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
